Netstock Data Engineer Challenge - Weather API
==============================================
Erhardt Nel

:Author Initials: Erhardt Nel
:toc:
:data-uri:
:icons:
:numbered:
:lang: en
:encoding: UTF-8


//////
:revinfo:
v1.0, Erhardt Nel, 2023-08-27:
 Document created
:legalnotice:
.Terms of use: 
This publication has been prepared for general guidance on matters of interest only.

:copyright: 2023, Netstock.
All rights reserved. No portion of this document may be
reproduced by any process without the written permission of NETSTOCK.

//////

Introduction
------------

This project focuses on a custom built ETL process to fetch weather data from the Open Meteo API, posts it periodically into a PostgreSQL table with the ability to perform delta changes. 

API Interface
-------------

The open-meteo.com weather API interface allow for hourly or daily weather forecasts. The decision was made to use the following global cities:

* Johannesburg - South Africa
* London - United Kingdom
* New York City - United States

Understanding the API Data
--------------------------

The Api interface allows for unauthenticated access, using a simple explorative tool like Microsoft Excel to connect to the API to understand and unpack the returned data structures.
This is achieved by setting up an excel web query to the API endpoint. 
It is important to add the Accept-Encoding gzip header to the request because the API returns a gzipped stream.

image::images/excel01.png[Excel GZIP header]

<<<
The Excel query returns multiple nodes in JSON format. The next step is to expand each individual Json Array into a separate sub query. 

image::images/excel03.png[]

We now have individual queries to all the JSON node arrays from the API result. These queries can be individually refreshed as data tables in Excel. 

image::images/excel04.png[]

<<<
This process allows me to quickly unpack what the data structures are in Excel Sheets.

image::images/excel05.png[]
image::images/excel06.png[] 

<<<
Fetch API data and create/update records in PostgreSQL
------------------------------------------------------

PostgreSQL Installation
~~~~~~~~~~~~~~~~~~~~~~~~

The following commands were used to install postgreSQL on my virtual machine.

[source, bash]
----
sudo apt install postgresql postgresql-contrib
----

For the purpose of this project I allowed any client from any ip to connect to the PostgreSQL server by editing the /etc/postgresql/15/main/postgresql.conf configuration file and adding the following line to it.

listen_addresses = '*'

Please see Production Ready section below on steps to harden the PostgreSQL installation.

Postgresql Setup
~~~~~~~~~~~~~~~~

The decision was made to post the weather API data to a local postgresql database hosted on a Debian Linux Virtual machine using two tables. 

.First - Temp Table: hld.weather
[source,sql]
----
CREATE SCHEMA IF NOT EXISTS TEMP; DROP TABLE IF EXISTS TEMP.hld_weather;
CREATE TABLE IF NOT EXISTS TEMP.hld_weather(
	id TEXT PRIMARY KEY NOT NULL, 
	city TEXT,
	dailytime TIMESTAMP, 
	temp_max DECIMAL, 
	temp_min DECIMAL, 
	sunset TIMESTAMP, 
	sunrise TIMESTAMP, 
	windspeed DECIMAL
);

----

.Second - Live Table: lve.weather
[source,sql]
----
CREATE SCHEMA IF NOT EXISTS TEMP; DROP TABLE IF EXISTS TEMP.lve_weather;
CREATE TABLE IF NOT EXISTS TEMP.lve_weather(
        id TEXT PRIMARY KEY NOT NULL,
        city TEXT,
        dailytime TIMESTAMP,
        temp_max DECIMAL,
        temp_min DECIMAL,
        sunset TIMESTAMP,
        sunrise TIMESTAMP,
        windspeed DECIMAL
);

----

<<<
The Temp table is used to push delta changes to the live table by using the native Postgresql *Merge* functionality.
This script will merge *HLD* table into the *LVE* table based on the ID column when matched and if not, it will insert a new record. 

.Merge SQL script
[source,sql]
----
merge into TEMP.lve_weather sda
using TEMP.hld_weather sdn
on sda.id = sdn.id
when matched then
  update set city = sdn.city, dailytime = sdn.dailytime, temp_max = sdn.temp_max, temp_min = sdn.temp_min, sunset = sdn.sunset, sunrise = sdn.sunrise, windspeed = sdn.windspeed
when not matched then
  insert (id, city, dailytime, temp_max,temp_min,sunset,sunrise,windspeed)
  values (sdn.id, sdn.city, sdn.dailytime, sdn.temp_max,sdn.temp_min,sdn.sunset,sdn.sunrise,sdn.windspeed);
----

<<<
Extract Scripts
~~~~~~~~~~~~~~~

The extract scripts were built using the KISS principle by limiting external dependencies on the virtual machine. 
To that extend I have used native Linux tools wrapped in a bash script to perform the following tasks:

1. Connect to the API using Curl.
2. Validate that the return string is valid JSON by using the jq Linux command.
3. Extract the JSON array values [ time, temperature_2m_max, temperature_2m_min, sunset, sunrise, windspeed_10m_max ] using the Linux jq command and adding the selectors.
4. The extracted values were then combined into a csv file for each of the cities.
5. The csv file is then posted to PostgreSQL using the *psql* command line syntax.
6. Errors and exceptions were written to a log.txt file.
7. A Scheduled cron job that runs hourly, connects to the API and update the SQL tables.

Extract Script Dependencies
+++++++++++++++++++++++++++

The following commands were used to install the dependencies on my virtual machine.

[source, bash]
----
sudo apt-get install jq
sudo apt-get install curl
----

<<<
.Script to fetch Johannesburg Data from API
[source, Bash]
----
#!/bin/bash

## This script fetches the different cities json data and check if valid json is returned.
## It then transforms the different columns and post the data to a postgresql temp table.
## Using the native postgresql merge statement to merge delta changes into the master table.

##------------------------------------------------------
##--- Global Variables ---
##------------------------------------------------------
CITY="latitude=-26.2023&longitude=28.0436"
STARTDATE="2023-06-01"
ENDDATE="2023-08-27"
WORKFOLDER=/media/sf_shared/netstock/postgresql
SQLLOG=$WORKFOLDER/log.txt
CITYNAME="Johannesburg"
##------------------------------------------------------

##-- Load the JSON from the webservice into a temp variable --
JSONRAW=$(curl -s "https://api.open-meteo.com/v1/forecast?$CITY&daily=temperature_2m_max,temperature_2m_min,sunrise,sunset,windspeed_10m_max&timezone=Africa%2FCairo&start_date=$STARTDATE&end_date=$ENDDATE")
##-- Cleanup the log file
touch $SQLLOG
rm $SQLLOG

##------------------------------------------------------
##-- Check if the returned JSON is valid  and transform the columns into csv --
if [ $(echo -n $JSONRAW | jq empty >  /dev/null 2>&1; echo $?) -eq 0 ]; then
        # Clean up the temp table
        psql -d netstock -a -f $WORKFOLDER/cleanup.sql >> $SQLLOG 2>&1
        echo $JSONRAW | jq '.daily.time[]' | tr -d '"' | awk '{print "JHB_"$1}' > $WORKFOLDER/id.csv
        echo $JSONRAW | jq '.daily.time[]' | tr -d '"' | awk '{print "Johannesburg"}' > $WORKFOLDER/city.csv
        echo $JSONRAW | jq '.daily.time[]' > $WORKFOLDER/timedaily.csv
        echo $JSONRAW | jq '.daily.temperature_2m_max[]' > $WORKFOLDER/tempmax.csv
        echo $JSONRAW | jq '.daily.temperature_2m_min[]' > $WORKFOLDER/tempmin.csv
        echo $JSONRAW | jq '.daily.sunset[]' > $WORKFOLDER/sunset.csv
        echo $JSONRAW | jq '.daily.sunrise[]' > $WORKFOLDER/sunrise.csv
        echo $JSONRAW | jq '.daily.windspeed_10m_max[]' > $WORKFOLDER/wind.csv
        touch $WORKFOLDER/out.csv
        rm $WORKFOLDER/out.csv
        #echo "dailytime,temperature_max,temperature_min,sunset,sunrise,windspeed" > out.csv
        paste -d',' $WORKFOLDER/id.csv $WORKFOLDER/city.csv $WORKFOLDER/timedaily.csv $WORKFOLDER/tempmax.csv $WORKFOLDER/tempmin.csv $WORKFOLDER/sunset.csv $WORKFOLDER/sunrise.csv $WORKFOLDER/wind.csv >>$WORKFOLDER/out.csv
        psql -d netstock -c "\copy temp.hld_weather from $WORKFOLDER/out.csv with delimiter ',' csv null as 'NULL';" >> $SQLLOG 2>&1
        #merge Delta changes into master table
        psql -d netstock -a -f $WORKFOLDER/merge.sql >> $SQLLOG 2>&1
else
        echo "json is invalid"
fi
##------------------------------------------------------
## Do a cleanup
rm $WORKFOLDER/*.csv
##------------------------------------------------------
----

<<<
.The output of the log.txt file contains the following information.
[source, SQL]
----
CREATE SCHEMA IF NOT EXISTS TEMP; DROP TABLE IF EXISTS TEMP.hld_weather;
psql:/media/sf_shared/netstock/postgresql/cleanup.sql:1: NOTICE:  schema "temp" already exists, skipping
CREATE SCHEMA
DROP TABLE
CREATE TABLE IF NOT EXISTS TEMP.hld_weather(id TEXT PRIMARY KEY NOT NULL, city text, dailytime TIMESTAMP, temp_max DECIMAL, temp_min DECIMAL, sunset TIMESTAMP, sunrise TIMESTAMP, windspeed DECIMAL);
CREATE TABLE
COPY 88
merge into TEMP.lve_weather sda
using TEMP.hld_weather sdn
on sda.id = sdn.id
when matched then
  update set city = sdn.city, dailytime = sdn.dailytime, temp_max = sdn.temp_max, temp_min = sdn.temp_min, sunset = sdn.sunset, sunrise = sdn.sunrise, windspeed = sdn.windspeed
when not matched then
  insert (id, city, dailytime, temp_max,temp_min,sunset,sunrise,windspeed)
  values (sdn.id, sdn.city, sdn.dailytime, sdn.temp_max,sdn.temp_min,sdn.sunset,sdn.sunrise,sdn.windspeed);
MERGE 88
----

<<<
Cron Job to load and merge data periodically
++++++++++++++++++++++++++++++++++++++++++++

I created a script called cronrun.sh that will collect the API data across the three cities.

.cronrun.sh
[source, BASH]
----
#!/bin/bash

## This scrpt is called by a cron job to connect to the weather API and post the various city data to Postgresql

##------------------------------------------------------
##--- Global Variables ---
##------------------------------------------------------
WORKFOLDER=/media/sf_shared/netstock/postgresql
/$WORKFOLDER/jhb.sh
/$WORKFOLDER/london.sh
/$WORKFOLDER/newyork.sh
----

I then installed an hourly cron job to fetch the data.

.CronJob
[source ,bash]
----
crontab -e

0 7-18 * * * /media/sf_shared/netstock/postgresql/cronrun.sh 2>&1
----


<<<
PostgreSQL Table Data
~~~~~~~~~~~~~~~~~~~~~

.Output of SQL table data
image::images/heidisql.png[]

<<<
Documentation Environment
-------------------------

Asciidoc
~~~~~~~~
I have used the Python based Asciidoc toolchain with the DBLatex rendering engine. Asciidoc allows you to add content in a text-based markup language. The tool chain allows you to convert your output to both PDF and HTML. 
The documentation source files are 100% text based which makes it ideal to put it under source control.

I use a custom MAKE file to convert asciidoc.txt file to PDF or HTML by issuing the command "make pdf" or "make html"

.Custom Make file
[source, bash]
----
TARGET=asciidoc

.PHONY: clean html xml pdf

html: $(TARGET).html

pdf: $(TARGET).pdf

all: html pdf

%.html: %.txt
        asciidoctor $<

%.pdf: %.txt        
        a2x --no-xmllint -v -a docinfo -fpdf --dblatex-opts="-s asciidoc-dblatex-custom.sty" --dblatex-opts="--param=doc.lot.show=figure,table"  $<        
        gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/printer -dNOPAUSE -dQUIET -dBATCH -sOutputFile=output.pdf asciidoc.pdf
clean:
        rm -f *~ $(TARGET).pdf $(TARGET).html docbook-xsl.css *.xml
----

Asciidoc Documentation
++++++++++++++++++++++

https://www.writethedocs.org/guide/writing/asciidoc[Asciidoc Markup Documentation]

Asciidoc Installation on Linux
++++++++++++++++++++++++++++++

[source, bash]
----
# Install asciidoc with the dblatex backend
sudo apt-get install asciidoc-dblatex

#install asciidoctor for the html backend
sudo apt-get install asciidoctor

----

<<<
Visualization of data using Microsoft Power BI Desktop
------------------------------------------------------

Power BI is configured to connect to the PostgreSQL server to retrieve the table data.

I created a simple vizualization to display the differences between the minimum and maximum temperatures across the three cities for one month.

.Power BI visualization
image::images/PowerBI.png[]


<<<
Steps needed to make the solution Prodution Ready
-------------------------------------------------

Basic Penetration Test
~~~~~~~~~~~~~~~~~~~~~~

A basic nmap scan against the virtual machine revealed port 22 [SSH] and 5432 [Postgresql] to be opened.

[source, bash]
----
sudo nmap 192.168.88.30 

Starting Nmap 7.93 ( https://nmap.org ) at 2023-08-28 00:54 SAST
Nmap scan report for 192.168.88.30
Host is up (0.000035s latency).

PORT     STATE SERVICE
22/tcp   open  ssh
5432/tcp open  postgresql

Nmap done: 1 IP address (1 host up) scanned in 0.11 seconds
----

The version of Postgresql is revealed as 15.3 by issuing the following command: psql -c "SELECT version();"

<<<
There are no known exploits listed for version 15.3 of postgresql

image::images/exploitdb1.png[]	

Steps to harden the Postgresql setup
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Port 5432 should not be exposed to the world. 
2. Only allow localhost to connect to the server
3. You can use SSH port forwarding to connect from the outside world.

Steps to harden SSH on the Linux server
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. No root access
2. Install fail2ban to allow lockout after two incorrect attempts.
3. Use certificate-based authentication vs username and password.

Alternatives to custom built solution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

1. Use automated pipelines for example Apache Airflow to extract the data.






