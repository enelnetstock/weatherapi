Netstock Data Engineer Challenge - Weather
==========================================
Erhardt Nel

:Author Initials: Erhardt Nel
:toc:
:data-uri:
:icons:
:numbered:
:lang: en
:encoding: UTF-8


//////
:revinfo:
v1.0, Erhardt Nel, 2023-08-27:
 Document created
:legalnotice:
.Terms of use: 
This publication has been prepared for general guidance on matters of interest only.

:copyright: 2023, Netstock.
All rights reserved. No portion of this document may be
reproduced by any process without the written permission of NETSTOCK.

//////

Introduction
------------

intro text here

API Interface
-------------

The open-meteo.com weather API interface allow for hourly or daily weather forecasts. The decision was made to use the following global cities:

* Johannesburg - South Africa
* London - United Kingdom
* New York City - United States

Undestanding the API Data
-------------------------

The Api interface allows for unauthenticated access. I will be using a simple explorative tool like Microsot Excel to onnect to to the API to understand and unpack the returned data structures.
This is achieved by setting up an excel web query to the API endpoint. 
It is important to add the Accept-Encoding gzip header to the request because the api returns a gzipped stream.

image::images/excel01.png[Excel GZIP header]

<<<
The Excel query returns mulliple nodes in JSON format. The next step is to expand each individual Json Array into a separate sub query. 

image::images/excel03.png[]

We now have individual queries to all the JSON node arrays from the API result. These queries can be individaully refreshed as data tables in Excel. 

image::images/excel04.png[]

<<<
This porcess allows me to quickly unpack what the data structures are in Excel Sheets.

image::images/excel05.png[]
image::images/excel06.png[] 

<<<
Connect to API and push data to Postgresql
------------------------------------------

Postgresql Installation
~~~~~~~~~~~~~~~~~~~~~~~~

The following commands were used to install postgresql on my virtual machine.

[source, bash]
----
sudo apt install postgresql postgresql-contrib
----

I then allowed any client from any ip to connect to the Postgresql server by editing the /etc/postgresql/15/main/postgresql.conf configuration file and adding the following line to it.

listen_addresses = '*'

Please see Prodution Ready section below on steps to harden the Postgresql installation.

Postgresql Setup
~~~~~~~~~~~~~~~~

The decision was made to post the weather API data to a local postgresql database hosted on a Debian Linux Virtual machine. 
I created two tables, 

.First - Temp Table: hld.weather
[source,sql]
----
CREATE SCHEMA IF NOT EXISTS TEMP; DROP TABLE IF EXISTS TEMP.hld_weather;
CREATE TABLE IF NOT EXISTS TEMP.hld_weather(
	id TEXT PRIMARY KEY NOT NULL, 
	city TEXT,
	dailytime TIMESTAMP, 
	temp_max DECIMAL, 
	temp_min DECIMAL, 
	sunset TIMESTAMP, 
	sunrise TIMESTAMP, 
	windspeed DECIMAL
);

----

.Second - Live Table: lve.weather
[source,sql]
----
CREATE SCHEMA IF NOT EXISTS TEMP; DROP TABLE IF EXISTS TEMP.lve_weather;
CREATE TABLE IF NOT EXISTS TEMP.lve_weather(
        id TEXT PRIMARY KEY NOT NULL,
        city TEXT,
        dailytime TIMESTAMP,
        temp_max DECIMAL,
        temp_min DECIMAL,
        sunset TIMESTAMP,
        sunrise TIMESTAMP,
        windspeed DECIMAL
);

----

The Temp table is used to push delta changes to the live table by using the native Postgresql Merge functionality.
This script will merge *HLD* table into the *LVE* table based on if the ID column is matched and if not it will insert a new record. 

.Merge SQL script
[source,sql]
----
merge into TEMP.lve_weather sda
using TEMP.hld_weather sdn
on sda.id = sdn.id
when matched then
  update set city = sdn.city, dailytime = sdn.dailytime, temp_max = sdn.temp_max, temp_min = sdn.temp_min, sunset = sdn.sunset, sunrise = sdn.sunrise, windspeed = sdn.windspeed
when not matched then
  insert (id, city, dailytime, temp_max,temp_min,sunset,sunrise,windspeed)
  values (sdn.id, sdn.city, sdn.dailytime, sdn.temp_max,sdn.temp_min,sdn.sunset,sdn.sunrise,sdn.windspeed);
----

<<<
Extract Scripts
~~~~~~~~~~~~~~~

The extract scripts were build with the KISS principle by limiting external dependencies on the virtual machine. 
To that extend I have used native linux tools wrapped in a bash script to perform the following tasks:

1. Connect to the API using Curl
2. Validate that the return string is valid JSON by using the jq linux command
3. Extract the JSON array values [ time, temperature_2m_max, temperature_2m_min, sunset, sunrise, windspeed_10m_max ] using the Linux jq command and adding the selectors.
4. The extracted values were then combined into a csv file for each of the cities.
5. The csv file is then posted to PostgreSQL using the *psql* commandline syntax.
6. Errors and exceptions were written to a log.txt file
7. A Scheduled cron job that runs hourly, connects to the API and update the SQL tables.

Extract Script Dependencies
+++++++++++++++++++++++++++

The following commands were used to install the dependencies on my virtual machine.

[source, bash]
----
sudo apt-get install jq
sudo apt-get install curl
----

<<<
.Script to fetch Johannesburg Data from API
[source, Bash]
----
#!/bin/bash

## This script fetch the different cities json data and check if valid json is returned.
## It then transform the different columns and post the data to a postgresql temp table.
## Using the native postgresql merge statment to merge delta changes into the master table.

##------------------------------------------------------
##--- Global Variables ---
##------------------------------------------------------
CITY="latitude=-26.2023&longitude=28.0436"
STARTDATE="2023-06-01"
ENDDATE="2023-08-27"
WORKFOLDER=/media/sf_shared/netstock/postgresql
SQLLOG=$WORKFOLDER/log.txt
CITYNAME="Johannesburg"
##------------------------------------------------------

##-- Load the JSON from the webservice into a temp variable --
JSONRAW=$(curl -s "https://api.open-meteo.com/v1/forecast?$CITY&daily=temperature_2m_max,temperature_2m_min,sunrise,sunset,windspeed_10m_max&timezone=Africa%2FCairo&start_date=$STARTDATE&end_date=$ENDDATE")
##-- Cleanup the log file
touch $SQLLOG
rm $SQLLOG

##------------------------------------------------------
##-- Check if the returned JSON is valid  and transform the columns into csv --
if [ $(echo -n $JSONRAW | jq empty >  /dev/null 2>&1; echo $?) -eq 0 ]; then
        # Clean up the temp table
        psql -d netstock -a -f $WORKFOLDER/cleanup.sql >> $SQLLOG 2>&1
        echo $JSONRAW | jq '.daily.time[]' | tr -d '"' | awk '{print "JHB_"$1}' > $WORKFOLDER/id.csv
        echo $JSONRAW | jq '.daily.time[]' | tr -d '"' | awk '{print "Johannesburg"}' > $WORKFOLDER/city.csv
        echo $JSONRAW | jq '.daily.time[]' > $WORKFOLDER/timedaily.csv
        echo $JSONRAW | jq '.daily.temperature_2m_max[]' > $WORKFOLDER/tempmax.csv
        echo $JSONRAW | jq '.daily.temperature_2m_min[]' > $WORKFOLDER/tempmin.csv
        echo $JSONRAW | jq '.daily.sunset[]' > $WORKFOLDER/sunset.csv
        echo $JSONRAW | jq '.daily.sunrise[]' > $WORKFOLDER/sunrise.csv
        echo $JSONRAW | jq '.daily.windspeed_10m_max[]' > $WORKFOLDER/wind.csv
        touch $WORKFOLDER/out.csv
        rm $WORKFOLDER/out.csv
        #echo "dailytime,temperature_max,temperature_min,sunset,sunrise,windspeed" > out.csv
        paste -d',' $WORKFOLDER/id.csv $WORKFOLDER/city.csv $WORKFOLDER/timedaily.csv $WORKFOLDER/tempmax.csv $WORKFOLDER/tempmin.csv $WORKFOLDER/sunset.csv $WORKFOLDER/sunrise.csv $WORKFOLDER/wind.csv >>$WORKFOLDER/out.csv
        psql -d netstock -c "\copy temp.hld_weather from $WORKFOLDER/out.csv with delimiter ',' csv null as 'NULL';" >> $SQLLOG 2>&1
        #merge Delta changes into master table
        psql -d netstock -a -f $WORKFOLDER/merge.sql >> $SQLLOG 2>&1
else
        echo "json is invalid"
fi
##------------------------------------------------------
## Do a cleanup
rm $WORKFOLDER/*.csv
##------------------------------------------------------
----

<<<
.The output of the log.txt file contains the following information.
[source, SQL]
----
CREATE SCHEMA IF NOT EXISTS TEMP; DROP TABLE IF EXISTS TEMP.hld_weather;
psql:/media/sf_shared/netstock/postgresql/cleanup.sql:1: NOTICE:  schema "temp" already exists, skipping
CREATE SCHEMA
DROP TABLE
CREATE TABLE IF NOT EXISTS TEMP.hld_weather(id TEXT PRIMARY KEY NOT NULL, city text, dailytime TIMESTAMP, temp_max DECIMAL, temp_min DECIMAL, sunset TIMESTAMP, sunrise TIMESTAMP, windspeed DECIMAL);
CREATE TABLE
COPY 88
merge into TEMP.lve_weather sda
using TEMP.hld_weather sdn
on sda.id = sdn.id
when matched then
  update set city = sdn.city, dailytime = sdn.dailytime, temp_max = sdn.temp_max, temp_min = sdn.temp_min, sunset = sdn.sunset, sunrise = sdn.sunrise, windspeed = sdn.windspeed
when not matched then
  insert (id, city, dailytime, temp_max,temp_min,sunset,sunrise,windspeed)
  values (sdn.id, sdn.city, sdn.dailytime, sdn.temp_max,sdn.temp_min,sdn.sunset,sdn.sunrise,sdn.windspeed);
MERGE 88
----

<<<
PostgreSQL Table Data
~~~~~~~~~~~~~~~~~~~~~

.Output of Live table
image::images/heidisql.png[]

<<<
Documentation Environment
-------------------------

Asciidoc
~~~~~~~~
I have used the Python based Asciidoc toolchain with the DBLatex rendering engine. Asciidoc allows you to add content in a text based markup language. The tool chain allows you to convert your output to both PDF and HTML. 
The documentation source files are 100% text based which makes it ideal to put it under source control.

I use a custom MAKE file to convert asciidoc.txt file to PDF or HTML by issuing the command "make pdf" or "make html"

.Custom Make file
[source, bash]
----
TARGET=asciidoc

.PHONY: clean html xml pdf

html: $(TARGET).html

pdf: $(TARGET).pdf

all: html pdf

%.html: %.txt
        asciidoctor $<

%.pdf: %.txt        
        a2x --no-xmllint -v -a docinfo -fpdf --dblatex-opts="-s asciidoc-dblatex-custom.sty" --dblatex-opts="--param=doc.lot.show=figure,table"  $<        
        gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/printer -dNOPAUSE -dQUIET -dBATCH -sOutputFile=output.pdf asciidoc.pdf
clean:
        rm -f *~ $(TARGET).pdf $(TARGET).html docbook-xsl.css *.xml
----

Asciidoc Documentation
++++++++++++++++++++++

https://www.writethedocs.org/guide/writing/asciidoc[Asciidoc Markup Documentation]

Asciidoc Installation on Linux
++++++++++++++++++++++++++++++

[source, bash]
----
# Install asciidoc with the dblatex backend
sudo apt-get install asciidoc-dblatex

#install asciidoctor for the html backend
sudo apt-get install asciidoctor

----

<<<
Elements needed to make the solution Prodution Ready
----------------------------------------------------

Basic Penetration Test
~~~~~~~~~~~~~~~~~~~~~~

A basic nmap scan against the virtual machine revealed port 22 [SSH] and 5432 [Postgresql] to be opened.

[source, bash]
----
sudo nmap 192.168.88.30 

Starting Nmap 7.93 ( https://nmap.org ) at 2023-08-28 00:54 SAST
Nmap scan report for 192.168.88.30
Host is up (0.000035s latency).

PORT     STATE SERVICE
22/tcp   open  ssh
5432/tcp open  postgresql

Nmap done: 1 IP address (1 host up) scanned in 0.11 seconds
----

The version of Postgresql is revealed as 15.3 by issuing the following command: psql -c "SELECT version();"

<<<
There are no known exploits listed for version 15.3 of postgresql

image::images/exploitdb1.png[]	

[index]
Index
-----


